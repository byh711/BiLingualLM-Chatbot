{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d9bbca",
   "metadata": {},
   "source": [
    "# Model Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e763195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFGPT2LMHeadModel\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "# Specify your filename path here\n",
    "filename = 'YOUR_FILE_PATH.xlsx'\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_excel(filename)\n",
    "\n",
    "train_data = pd.DataFrame(columns=['Question','Answer'])\n",
    "ind = 0\n",
    "\n",
    "for i in range(len(df)):\n",
    "    for j in range(3):\n",
    "        if pd.isna(df.Question[i]) != True and pd.isna(df.iloc[i,j+1]) != True:\n",
    "            train_data = pd.concat([train_data, pd.DataFrame({\"Question\":df.Question[i],\"Answer\":df.iloc[i,j+1]}, index=[ind])], axis=0)\n",
    "            ind += 1\n",
    "\n",
    "train_data = train_data.dropna()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd1e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small', padding_side=\"left\")\n",
    "\n",
    "# Check if the tokenizer has a pad token, sep token and eos token. If not, choose one.\n",
    "if not tokenizer.pad_token:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "if not tokenizer.sep_token:\n",
    "    tokenizer.sep_token = tokenizer.eos_token\n",
    "if not tokenizer.eos_token:\n",
    "    tokenizer.eos_token = ''\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = TFGPT2LMHeadModel.from_pretrained('microsoft/DialoGPT-small')\n",
    "\n",
    "# Ensure that the model and tokenizer have the same number of tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "def get_chat_data():\n",
    "    for question, answer in zip(train_data.Question.to_list(), train_data.Answer.to_list()):\n",
    "        if question is not None and answer is not None:  # Add this check\n",
    "            sent = tokenizer.encode(question + tokenizer.sep_token + answer + tokenizer.eos_token, add_special_tokens=True) \n",
    "            yield sent\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(get_chat_data, output_types=tf.int32)\n",
    "dataset = dataset.padded_batch(batch_size=batch_size, padded_shapes=(None,), padding_values=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9ea91d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n",
    "steps = len(train_data) // batch_size + 1\n",
    "EPOCHS = 300\n",
    "\n",
    "min_epoch_loss = np.inf\n",
    "patience = 10\n",
    "wait = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in tqdm.tqdm(dataset, total=steps):\n",
    "        with tf.GradientTape() as tape:\n",
    "            result = model(batch, labels=batch)\n",
    "            loss = result[0]\n",
    "            batch_loss = tf.reduce_mean(loss)\n",
    "          \n",
    "        grads = tape.gradient(batch_loss, model.trainable_variables)\n",
    "        adam.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        epoch_loss += batch_loss / steps\n",
    "\n",
    "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, epoch_loss))\n",
    "\n",
    "    if epoch_loss < min_epoch_loss:\n",
    "        min_epoch_loss = epoch_loss\n",
    "        wait = 0\n",
    "        \n",
    "        tokenizer.save_pretrained(\"model_checkpoint/dialoGPT_ENG\")\n",
    "        model.save_pretrained(\"model_checkpoint/dialoGPT_ENG\")\n",
    "        print(\"Saved model & tokenizer at epoch\", epoch + 1)\n",
    "    else:\n",
    "        wait += 1\n",
    "\n",
    "    if wait >= patience:\n",
    "        print(\"Early stopping at epoch\", epoch + 1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91234f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_answer_by_chatbot(user_text):\n",
    "    # add_special_tokens=True automatically creates the attention_mask\n",
    "    input_ids = tokenizer.encode(user_text + tokenizer.eos_token, return_tensors='tf', add_special_tokens=True)\n",
    "    \n",
    "    # generate the mask based on the input_ids\n",
    "    attention_mask = tf.where(input_ids != tokenizer.pad_token_id, 1, 0)\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids, \n",
    "        max_length=100, \n",
    "        do_sample=True, \n",
    "        top_p=0.9, \n",
    "        top_k=20, \n",
    "        num_return_sequences=1, \n",
    "        early_stopping=True, \n",
    "        pad_token_id=tokenizer.eos_token_id, # Set pad_token_id to eos_token_id\n",
    "        attention_mask=attention_mask, # Set the attention_mask\n",
    "    )\n",
    "    sentence = tokenizer.decode(output[0].numpy().tolist(), skip_special_tokens=True)\n",
    "    chatbot_response = sentence[len(user_text):]  # Remove the user text from the start of the response\n",
    "    return chatbot_response.strip()  # strip() is used to remove leading and trailing white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aecbc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_answer_by_chatbot('I am hungry')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1237fc8",
   "metadata": {},
   "source": [
    "# Convert Model to ONNX Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d22533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFGPT2LMHeadModel\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "\n",
    "onnxruntime.set_default_logger_severity(3)\n",
    "\n",
    "onnx_session = onnxruntime.InferenceSession('onnx_model/dialoGPT_ENG/decoder_model.onnx')\n",
    "tokenizer = AutoTokenizer.from_pretrained('onnx_model/dialoGPT_ENG')\n",
    "\n",
    "output_names = [output.name for output in onnx_session.get_outputs()]\n",
    "print(\"Output names:\", output_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc7f4bc",
   "metadata": {},
   "source": [
    "### (1) Greedy Search Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0628e023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(input_ids, attention_mask, max_length=50):\n",
    "    current_length = input_ids.shape[1]\n",
    "    \n",
    "    while current_length < max_length:\n",
    "        outputs = onnx_session.run(output_names=['logits'], input_feed={\"input_ids\": input_ids, \"attention_mask\": attention_mask})\n",
    "        logits = outputs[0]\n",
    "        predicted_token_id = np.argmax(logits, axis=-1)[0, -1]\n",
    "        \n",
    "        # Break if the EOS token is generated\n",
    "        if predicted_token_id == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        # Update input_ids and attention_mask\n",
    "        input_ids = np.concatenate((input_ids, np.array([[predicted_token_id]])), axis=1)\n",
    "        attention_mask = np.concatenate((attention_mask, np.array([[1]])), axis=1)\n",
    "        \n",
    "        current_length += 1\n",
    "\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a60e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    user_text = input(\"Question: \")\n",
    "    start_time = time.time() \n",
    "\n",
    "    # Note that we use the user text directly, not wrapped in <usr> or <sys> tags, \n",
    "    # as GPT2 doesn't use them. The eos_token is used to mark the end of the question.\n",
    "    input_ids = tokenizer.encode(user_text + tokenizer.eos_token, return_tensors='np').astype(np.int64)\n",
    "    \n",
    "    # Create attention mask\n",
    "    attention_mask = np.ones_like(input_ids, dtype=np.int64)\n",
    "\n",
    "    decoded_input_ids = greedy_decode(input_ids, attention_mask)\n",
    "    decoded_text = tokenizer.decode(decoded_input_ids[0])\n",
    "\n",
    "    # Strip the question text and the eos_token from the response\n",
    "    decoded_text = decoded_text[len(user_text)+len(tokenizer.eos_token):]\n",
    "    \n",
    "    print(f\"Answer: {decoded_text}\")\n",
    "\n",
    "    finish_time = time.time() - start_time\n",
    "    print(f\"runtime: {finish_time}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d8e0d",
   "metadata": {},
   "source": [
    "### (2) Top-K Sampling Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc4ff43",
   "metadata": {},
   "source": [
    "In this method, the top k most probable next tokens are filtered and the probability mass is redistributed amongst these tokens. This implies that the number of candidate tokens is always the same, regardless of the distribution of probabilities. For instance, even if a single token has a 95% probability of being the next token, top-k will still consider 99 other possibilities (if k is set to 100), making the sampling process more random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7984cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    e_x = np.exp(logits - np.max(logits))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def top_k_sampling(logits, k=10):\n",
    "    indices_to_remove = logits < np.sort(logits)[-k]\n",
    "    logits[indices_to_remove] = -np.finfo(np.float32).max\n",
    "    probs = softmax(logits)\n",
    "    token_id = np.random.choice(len(logits), size=1, p=probs)[0]\n",
    "    return token_id\n",
    "\n",
    "def top_k_sampling_decode(input_ids, attention_mask, max_length=50, k=10):\n",
    "    current_length = input_ids.shape[1]\n",
    "\n",
    "    while current_length < max_length:\n",
    "        outputs = onnx_session.run(output_names=['logits'], input_feed={\"input_ids\": input_ids, \"attention_mask\": attention_mask})\n",
    "        logits = outputs[0][0, -1, :]\n",
    "\n",
    "        # Apply top-k sampling\n",
    "        predicted_token_id = top_k_sampling(logits, k=k)\n",
    "\n",
    "        # Break if the EOS token is generated\n",
    "        if predicted_token_id == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        # Update input_ids and attention_mask\n",
    "        input_ids = np.concatenate((input_ids, np.array([[predicted_token_id]])), axis=1).astype(np.int64)\n",
    "        attention_mask = np.concatenate((attention_mask, np.array([[1]])), axis=1).astype(np.int64)\n",
    "        \n",
    "        current_length += 1\n",
    "\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0e9f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    user_text = input(\"Question: \")\n",
    "    start_time = time.time() \n",
    "    \n",
    "    # Note that we use the user text directly, not wrapped in <usr> or <sys> tags, \n",
    "    # as GPT2 doesn't use them. The eos_token is used to mark the end of the question.\n",
    "    input_ids = tokenizer.encode(user_text + tokenizer.eos_token, return_tensors='np').astype(np.int64)\n",
    "    \n",
    "    # Create attention mask\n",
    "    attention_mask = np.ones_like(input_ids, dtype=np.int64)\n",
    "\n",
    "    decoded_input_ids = top_k_sampling_decode(input_ids, attention_mask)\n",
    "    decoded_text = tokenizer.decode(decoded_input_ids[0])\n",
    "\n",
    "    # Strip the question text and the eos_token from the response\n",
    "    decoded_text = decoded_text[len(user_text)+len(tokenizer.eos_token):]\n",
    "    \n",
    "    print(f\"Answer: {decoded_text}\")\n",
    "\n",
    "    finish_time = time.time() - start_time\n",
    "    print(f\"runtime: {finish_time}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce63ef7",
   "metadata": {},
   "source": [
    "### (3) Top-P Sampling Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9600411",
   "metadata": {},
   "source": [
    "top-p sampling chooses the smallest set of tokens so that the sum of their probabilities is greater than p. The selected tokens are then renormalized to sum to one. In contrast to top-k, the number of tokens considered for the next step can change dynamically based on the output of the model. For example, if the model is very certain about its next token (i.e., one token has a probability of 95% to be next), it will likely choose this one token (if p is set to 0.95 or higher). So, top-p can be seen as a dynamic version of top-k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96c8296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "def top_p_sampling(logits, p=0.9):\n",
    "    sorted_logits = np.sort(logits)[::-1]\n",
    "    sorted_indices = np.argsort(logits)[::-1]\n",
    "\n",
    "    cumulative_probs = np.cumsum(softmax(sorted_logits))\n",
    "\n",
    "    indices_to_remove = cumulative_probs > p\n",
    "    indices_to_remove = np.roll(indices_to_remove, 1)  # Shift the indices_to_remove by one to the right to keep the top token\n",
    "    indices_to_remove[0] = False  # Set the first token as False to always keep it\n",
    "\n",
    "    logits[sorted_indices[indices_to_remove]] = -np.inf\n",
    "    probs = softmax(logits)\n",
    "\n",
    "    token_id = np.random.choice(len(logits), size=1, p=probs)[0]\n",
    "\n",
    "    return token_id\n",
    "\n",
    "def top_p_sampling_decode(input_ids, attention_mask, max_length=50, p=0.9):\n",
    "    current_length = input_ids.shape[1]\n",
    "\n",
    "    while current_length < max_length:\n",
    "        outputs = onnx_session.run(output_names=['logits'], input_feed={\"input_ids\": input_ids, \"attention_mask\": attention_mask})\n",
    "        logits = outputs[0][0, -1, :]\n",
    "\n",
    "        # Apply Top-p sampling\n",
    "        predicted_token_id = top_p_sampling(logits, p=p)\n",
    "\n",
    "        # Break if the EOS token is generated\n",
    "        if predicted_token_id == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        # Update input_ids and attention_mask\n",
    "        input_ids = np.concatenate((input_ids, np.array([[predicted_token_id]])), axis=1).astype(np.int64)\n",
    "        attention_mask = np.concatenate((attention_mask, np.array([[1]])), axis=1).astype(np.int64)\n",
    "\n",
    "        current_length += 1\n",
    "\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f97c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    user_text = input(\"Question: \")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Note that we use the user text directly, not wrapped in <usr> or <sys> tags, \n",
    "    # as GPT2 doesn't use them. The eos_token is used to mark the end of the question.\n",
    "    input_ids = tokenizer.encode(user_text + tokenizer.eos_token, return_tensors='np').astype(np.int64)\n",
    "\n",
    "    # Create attention mask\n",
    "    attention_mask = np.ones_like(input_ids, dtype=np.int64)\n",
    "\n",
    "    decoded_input_ids = top_p_sampling_decode(input_ids, attention_mask)\n",
    "    decoded_text = tokenizer.decode(decoded_input_ids[0])\n",
    "\n",
    "    # Strip the question text and the eos_token from the response\n",
    "    decoded_text = decoded_text[len(user_text)+len(tokenizer.eos_token):]\n",
    "\n",
    "    print(f\"Answer: {decoded_text}\")\n",
    "\n",
    "    finish_time = time.time() - start_time\n",
    "    print(f\"runtime: {finish_time}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eea0ef",
   "metadata": {},
   "source": [
    "### (4) Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661ad0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(input_ids, attention_mask, beam_size=2, max_length=50):\n",
    "    input_ids = input_ids.tolist()[0]\n",
    "    attention_mask = attention_mask.tolist()[0]\n",
    "\n",
    "    finished_beams = []\n",
    "    running_beam = [(0, input_ids, attention_mask)]\n",
    "    \n",
    "    while len(finished_beams) < beam_size and running_beam:\n",
    "        beam_score, input_ids, attention_mask = running_beam.pop(0)\n",
    "        input_ids_np = np.array([input_ids], dtype=np.int64)\n",
    "        attention_mask_np = np.array([attention_mask], dtype=np.int64)\n",
    "        \n",
    "        outputs = onnx_session.run(output_names=['logits'], input_feed={\"input_ids\": input_ids_np, \"attention_mask\": attention_mask_np})\n",
    "        logits = outputs[0][0, -1, :]\n",
    "\n",
    "        # Choose top 2 (beam_size) tokens\n",
    "        top_k_logits = np.sort(logits)[-beam_size:]\n",
    "        top_k_tokens = np.argsort(logits)[-beam_size:]\n",
    "\n",
    "        for i in range(beam_size):\n",
    "            token = top_k_tokens[i]\n",
    "            score = top_k_logits[i]\n",
    "            \n",
    "            # Add the new token and update attention_mask\n",
    "            new_input_ids = input_ids + [token]\n",
    "            new_attention_mask = attention_mask + [1]\n",
    "\n",
    "            if token == tokenizer.eos_token_id or len(new_input_ids) == max_length:\n",
    "                finished_beams.append((beam_score + score, new_input_ids, new_attention_mask))\n",
    "            else:\n",
    "                running_beam.append((beam_score + score, new_input_ids, new_attention_mask))\n",
    "                \n",
    "        # Sort the running beams by score\n",
    "        running_beam.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # Return the highest scoring finished beam\n",
    "    return max(finished_beams, key=lambda x: x[0])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a9a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    user_text = input(\"Question: \")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Note that we use the user text directly, not wrapped in <usr> or <sys> tags, \n",
    "    # as GPT2 doesn't use them. The eos_token is used to mark the end of the question.\n",
    "    input_ids = tokenizer.encode(user_text + tokenizer.eos_token, return_tensors='np').astype(np.int64)\n",
    "\n",
    "    # Create attention mask\n",
    "    attention_mask = np.ones_like(input_ids, dtype=np.int64)\n",
    "\n",
    "    decoded_input_ids = beam_search(input_ids, attention_mask)\n",
    "    decoded_text = tokenizer.decode(decoded_input_ids)\n",
    "\n",
    "    # Strip the question text and the eos_token from the response\n",
    "    decoded_text = decoded_text[len(user_text)+len(tokenizer.eos_token):-len(tokenizer.eos_token)]\n",
    "\n",
    "    print(f\"Answer: {decoded_text}\")\n",
    "\n",
    "    finish_time = time.time() - start_time\n",
    "    print(f\"runtime: {finish_time}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
