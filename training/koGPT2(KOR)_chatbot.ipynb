{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d9bbca",
   "metadata": {},
   "source": [
    "# Model Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e763195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer,TFGPT2LMHeadModel\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Specify your filename path here\n",
    "filename = 'YOUR_FILE_PATH.xlsx'\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_excel(filename)\n",
    "\n",
    "train_data = pd.DataFrame(columns=['Question','Answer'])\n",
    "ind = 0\n",
    "\n",
    "for i in range(len(df)):\n",
    "    for j in range(3):\n",
    "        if pd.isna(df.Question[i]) != True and pd.isna(df.iloc[i,j+1]) != True:\n",
    "            train_data = pd.concat([train_data, pd.DataFrame({\"Question\":df.Question[i],\"Answer\":df.iloc[i,j+1]}, index=[ind])], axis=0)\n",
    "            ind += 1\n",
    "\n",
    "train_data = train_data.dropna()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd1e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', pad_token='<pad>')\n",
    "model = TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "def get_chat_data():\n",
    "    for question, answer in zip(train_data.Question.to_list(), train_data.Answer.to_list()):\n",
    "        bos_token = [tokenizer.bos_token_id]\n",
    "        eos_token = [tokenizer.eos_token_id]\n",
    "        sent = tokenizer.encode('<usr>' + question + '<sys>' + answer) \n",
    "        yield bos_token + sent + eos_token\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(get_chat_data, output_types=tf.int32)\n",
    "dataset = dataset.padded_batch(batch_size=batch_size, padded_shapes=(None,), padding_values=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9ea91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n",
    "steps = len(train_data) // batch_size + 1\n",
    "EPOCHS = 300\n",
    "\n",
    "# New variables for early stopping\n",
    "min_epoch_loss = np.inf\n",
    "patience = 10\n",
    "wait = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in tqdm.tqdm(dataset, total=steps):\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            result = model(batch, labels=batch)\n",
    "            loss = result[0]\n",
    "            batch_loss = tf.reduce_mean(loss)\n",
    "          \n",
    "        grads = tape.gradient(batch_loss, model.trainable_variables)\n",
    "        adam.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        epoch_loss += batch_loss / steps\n",
    "\n",
    "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, epoch_loss))\n",
    "    \n",
    "    # If epoch loss is lower than the current minimum, update the minimum\n",
    "    if epoch_loss < min_epoch_loss:\n",
    "        min_epoch_loss = epoch_loss\n",
    "        wait = 0  # Reset the counter\n",
    "        \n",
    "        tokenizer.save_pretrained(\"model_checkpoint/kogpt2_KOR\")\n",
    "        model.save_pretrained(\"model_checkpoint/kogpt2_KOR\")\n",
    "        print(\"Saved model & tokenizer at epoch\", epoch + 1)\n",
    "    else:\n",
    "        wait += 1  # Increment the counter\n",
    "\n",
    "    # If the counter has reached the limit, stop the training\n",
    "    if wait >= patience:\n",
    "        print(\"Early stopping at epoch\", epoch + 1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91234f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_answer_by_chatbot(user_text):\n",
    "    sent = '<usr>' + user_text + '<sys>'\n",
    "    input_ids = [tokenizer.bos_token_id] + tokenizer.encode(sent)\n",
    "    input_ids = tf.convert_to_tensor([input_ids])\n",
    "    output = model.generate(input_ids, max_length=20, do_sample=True, top_p=0.9, top_k=20, num_return_sequences=1,\n",
    "                       early_stopping=True, eos_token_id=tokenizer.eos_token_id)\n",
    "    sentence = tokenizer.decode(output[0].numpy().tolist())\n",
    "    chatbot_response = sentence.split('<sys> ')[1].replace('</s>', '')\n",
    "    return chatbot_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aecbc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_answer_by_chatbot('도끼는 어떻게 만들어?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1237fc8",
   "metadata": {},
   "source": [
    "# Convert Model to ONNX Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d22533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnxruntime \n",
    "from transformers import AutoTokenizer\n",
    "import onnxruntime as ort\n",
    "\n",
    "ort.set_default_logger_severity(3)\n",
    "\n",
    "onnx_session = onnxruntime.InferenceSession('onnx_model/kogpt2_KOR/decoder_model.onnx')\n",
    "tokenizer = AutoTokenizer.from_pretrained('onnx_model/kogpt2_KOR', bos_token='</s>', eos_token='</s>', pad_token='<pad>')\n",
    "\n",
    "output_names = [output.name for output in onnx_session.get_outputs()]\n",
    "print(\"Output names:\", output_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc7f4bc",
   "metadata": {},
   "source": [
    "### (1) Greedy Search Method¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0628e023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(input_ids, attention_mask, max_length=50):\n",
    "    current_length = input_ids.shape[1]\n",
    "    \n",
    "    while current_length < max_length:\n",
    "        outputs = onnx_session.run(output_names=['logits'], input_feed={\"input_ids\": input_ids, \"attention_mask\": attention_mask})\n",
    "        logits = outputs[0]\n",
    "        predicted_token_id = np.argmax(logits, axis=-1)[0, -1]\n",
    "        \n",
    "        # Break if the EOS token is generated\n",
    "        if predicted_token_id == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        # Update input_ids and attention_mask\n",
    "        input_ids = np.concatenate((input_ids, np.array([[predicted_token_id]])), axis=1)\n",
    "        attention_mask = np.concatenate((attention_mask, np.array([[1]])), axis=1)\n",
    "        \n",
    "        current_length += 1\n",
    "\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a60e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    user_text = input(\"질문: \")\n",
    "    start_time = time.time() \n",
    "    \n",
    "    sent = '<usr>' + user_text + '<sys>'\n",
    "    input_ids = [tokenizer.bos_token_id] + tokenizer.encode(sent)\n",
    "    input_ids = np.array([input_ids], dtype=np.int64)\n",
    "\n",
    "    # Create attention mask\n",
    "    attention_mask = np.ones_like(input_ids, dtype=np.int64)\n",
    "\n",
    "    decoded_input_ids = greedy_decode(input_ids, attention_mask)\n",
    "    decoded_text = tokenizer.decode(decoded_input_ids[0])\n",
    "    decoded_text = decoded_text.split('<sys> ')[1].replace('</s>', '')\n",
    "    print(f\"답변: {decoded_text}\")\n",
    "\n",
    "    finish_time = time.time() - start_time\n",
    "    print(f\"runtime: {finish_time}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63db3d47",
   "metadata": {},
   "source": [
    "### (2) Top-K Sampling Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c4835b",
   "metadata": {},
   "source": [
    "In this method, the top k most probable next tokens are filtered and the probability mass is redistributed amongst these tokens. This implies that the number of candidate tokens is always the same, regardless of the distribution of probabilities. For instance, even if a single token has a 95% probability of being the next token, top-k will still consider 99 other possibilities (if k is set to 100), making the sampling process more random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9075e217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "def top_k_sampling(logits, k=10):\n",
    "    indices_to_remove = logits < np.sort(logits)[-k]\n",
    "    logits[indices_to_remove] = -np.finfo(np.float32).max\n",
    "    probs = softmax(logits)\n",
    "    token_id = np.random.choice(len(logits), size=1, p=probs)[0]\n",
    "    return token_id\n",
    "\n",
    "def top_k_sampling_decode(input_ids, attention_mask, max_length=50, k=10):\n",
    "    current_length = input_ids.shape[1]\n",
    "\n",
    "    while current_length < max_length:\n",
    "        outputs = onnx_session.run(output_names=['logits'], input_feed={\"input_ids\": input_ids, \"attention_mask\": attention_mask})\n",
    "        logits = outputs[0][0, -1, :]\n",
    "\n",
    "        # Apply top-k sampling\n",
    "        predicted_token_id = top_k_sampling(logits, k=k)\n",
    "\n",
    "        # Break if the EOS token is generated\n",
    "        if predicted_token_id == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        # Update input_ids and attention_mask\n",
    "        input_ids = np.concatenate((input_ids, np.array([[predicted_token_id]])), axis=1).astype(np.int64)\n",
    "        attention_mask = np.concatenate((attention_mask, np.array([[1]])), axis=1).astype(np.int64)\n",
    "        \n",
    "        current_length += 1\n",
    "\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d04b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    user_text = input(\"질문: \")\n",
    "    start_time = time.time() \n",
    "    \n",
    "    sent = '<usr>' + user_text + '<sys>'\n",
    "    input_ids = [tokenizer.bos_token_id] + tokenizer.encode(sent)\n",
    "    input_ids = np.array([input_ids], dtype=np.int64)\n",
    "\n",
    "    # Create attention mask\n",
    "    attention_mask = np.ones_like(input_ids, dtype=np.int64)\n",
    "\n",
    "    decoded_input_ids = top_k_sampling_decode(input_ids, attention_mask)\n",
    "    decoded_text = tokenizer.decode(decoded_input_ids[0])\n",
    "    decoded_text = decoded_text.split('<sys> ')[1].replace('</s>', '')\n",
    "    print(f\"답변: {decoded_text}\")\n",
    "\n",
    "    finish_time = time.time() - start_time\n",
    "    print(f\"runtime: {finish_time}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba72330",
   "metadata": {},
   "source": [
    "### (3) Top-P Sampling Method¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6a8be0",
   "metadata": {},
   "source": [
    "top-p sampling chooses the smallest set of tokens so that the sum of their probabilities is greater than p. The selected tokens are then renormalized to sum to one. In contrast to top-k, the number of tokens considered for the next step can change dynamically based on the output of the model. For example, if the model is very certain about its next token (i.e., one token has a probability of 95% to be next), it will likely choose this one token (if p is set to 0.95 or higher). So, top-p can be seen as a dynamic version of top-k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f8312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "def top_p_sampling(logits, p=0.9):\n",
    "    sorted_logits = np.sort(logits)[::-1]\n",
    "    sorted_indices = np.argsort(logits)[::-1]\n",
    "\n",
    "    cumulative_probs = np.cumsum(softmax(sorted_logits))\n",
    "\n",
    "    indices_to_remove = cumulative_probs > p\n",
    "    indices_to_remove = np.roll(indices_to_remove, 1)  # Shift the indices_to_remove by one to the right to keep the top token\n",
    "    indices_to_remove[0] = False  # Set the first token as False to always keep it\n",
    "\n",
    "    logits[sorted_indices[indices_to_remove]] = -np.inf\n",
    "    probs = softmax(logits)\n",
    "\n",
    "    token_id = np.random.choice(len(logits), size=1, p=probs)[0]\n",
    "\n",
    "    return token_id\n",
    "\n",
    "def top_p_sampling_decode(input_ids, attention_mask, max_length=50, p=0.9):\n",
    "    current_length = input_ids.shape[1]\n",
    "\n",
    "    while current_length < max_length:\n",
    "        outputs = onnx_session.run(output_names=['logits'], input_feed={\"input_ids\": input_ids, \"attention_mask\": attention_mask})\n",
    "        logits = outputs[0][0, -1, :]\n",
    "\n",
    "        # Apply Top-p sampling\n",
    "        predicted_token_id = top_p_sampling(logits, p=p)\n",
    "\n",
    "        # Break if the EOS token is generated\n",
    "        if predicted_token_id == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        # Update input_ids and attention_mask\n",
    "        input_ids = np.concatenate((input_ids, np.array([[predicted_token_id]])), axis=1).astype(np.int64)\n",
    "        attention_mask = np.concatenate((attention_mask, np.array([[1]])), axis=1).astype(np.int64)\n",
    "\n",
    "        current_length += 1\n",
    "\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a7c6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    user_text = input(\"질문: \")\n",
    "    start_time = time.time() \n",
    "    \n",
    "    sent = '<usr>' + user_text + '<sys>'\n",
    "    input_ids = [tokenizer.bos_token_id] + tokenizer.encode(sent)\n",
    "    input_ids = np.array([input_ids], dtype=np.int64)\n",
    "\n",
    "    # Create attention mask\n",
    "    attention_mask = np.ones_like(input_ids, dtype=np.int64)\n",
    "\n",
    "    decoded_input_ids = top_p_sampling_decode(input_ids, attention_mask)\n",
    "    decoded_text = tokenizer.decode(decoded_input_ids[0])\n",
    "    decoded_text = decoded_text.split('<sys> ')[1].replace('</s>', '')\n",
    "    print(f\"답변: {decoded_text}\")\n",
    "\n",
    "    finish_time = time.time() - start_time\n",
    "    print(f\"runtime: {finish_time}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eea0ef",
   "metadata": {},
   "source": [
    "### (4) Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661ad0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(input_ids, attention_mask, beam_size=2, max_length=50):\n",
    "    input_ids = input_ids.tolist()[0]\n",
    "    attention_mask = attention_mask.tolist()[0]\n",
    "\n",
    "    finished_beams = []\n",
    "    running_beam = [(0, input_ids, attention_mask)]\n",
    "    \n",
    "    while len(finished_beams) < beam_size and running_beam:\n",
    "        beam_score, input_ids, attention_mask = running_beam.pop(0)\n",
    "        input_ids_np = np.array([input_ids], dtype=np.int64)\n",
    "        attention_mask_np = np.array([attention_mask], dtype=np.int64)\n",
    "        \n",
    "        outputs = onnx_session.run(output_names=['logits'], input_feed={\"input_ids\": input_ids_np, \"attention_mask\": attention_mask_np})\n",
    "        logits = outputs[0][0, -1, :]\n",
    "\n",
    "        # Choose top 2 (beam_size) tokens\n",
    "        top_k_logits = np.sort(logits)[-beam_size:]\n",
    "        top_k_tokens = np.argsort(logits)[-beam_size:]\n",
    "\n",
    "        for i in range(beam_size):\n",
    "            token = top_k_tokens[i]\n",
    "            score = top_k_logits[i]\n",
    "            \n",
    "            # Add the new token and update attention_mask\n",
    "            new_input_ids = input_ids + [token]\n",
    "            new_attention_mask = attention_mask + [1]\n",
    "\n",
    "            if token == tokenizer.eos_token_id or len(new_input_ids) == max_length:\n",
    "                finished_beams.append((beam_score + score, new_input_ids, new_attention_mask))\n",
    "            else:\n",
    "                running_beam.append((beam_score + score, new_input_ids, new_attention_mask))\n",
    "                \n",
    "        # Sort the running beams by score\n",
    "        running_beam.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # Return the highest scoring finished beam\n",
    "    return max(finished_beams, key=lambda x: x[0])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a9a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    user_text = input(\"질문: \")\n",
    "    start_time = time.time() \n",
    "\n",
    "    sent = '<usr>' + user_text + '<sys>'\n",
    "    input_ids = [tokenizer.bos_token_id] + tokenizer.encode(sent)\n",
    "    input_ids = np.array([input_ids], dtype=np.int64)\n",
    "\n",
    "    # Create attention mask\n",
    "    attention_mask = np.ones_like(input_ids, dtype=np.int64)\n",
    "\n",
    "    decoded_input_ids = beam_search(input_ids, attention_mask)\n",
    "    decoded_text = tokenizer.decode(decoded_input_ids)\n",
    "    decoded_text = decoded_text.split('<sys> ')[1].replace('</s>', '')\n",
    "\n",
    "    print(f\"답변: {decoded_text}\")\n",
    "\n",
    "    finish_time = time.time() - start_time\n",
    "    print(f\"runtime: {finish_time}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
